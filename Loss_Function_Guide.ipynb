{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnyanshwalwadkar/Adv-Deep-Learning/blob/main/Loss_Function_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVWC8dR-NAKh"
      },
      "source": [
        "# Title: The Art of Loss Function Selection in Deep Learning: A Comprehensive Guide\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgH1GFmKHLo-"
      },
      "source": [
        "# Mean Squared Error (MSE) Loss:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gSBE0dcI2a3"
      },
      "source": [
        "MSE is a widely used loss function for regression problems. It calculates the average squared difference between the true and predicted values. This loss function is sensitive to outliers, as the error gets squared, making it suitable for problems where larger errors should be penalized more heavily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKo1Fe4_GXw3",
        "outputId": "1970aa52-0fc7-4148-c8f0-bbd992269f6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE: 0.02200000000000002\n"
          ]
        }
      ],
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    mse = sum((y_true[i] - y_pred[i])**2 for i in range(n)) / n\n",
        "    return mse\n",
        "\n",
        "y_true = [1, 2, 3, 4, 5]\n",
        "y_pred = [1.1, 1.9, 3.2, 3.8, 5.1]\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "print(\"MSE:\", mse)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mean Squared Error (MSE) is sensitive to outliers because it squares the differences between the predicted values and the actual values. Outliers, by definition, are data points that deviate significantly from the overall pattern in the dataset. When the error associated with an outlier is squared, it is magnified, causing the MSE to be disproportionately influenced by these extreme values. Consequently, a model optimized using MSE is likely to be overly sensitive to outliers, which can negatively impact its performance on the majority of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE without outliers: 0.02200000000000002\n",
            "MSE with outliers: 266.685\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulated true values and predicted values\n",
        "true_values = np.array([1, 2, 3, 4, 5])\n",
        "predicted_values = np.array([1.1, 1.9, 3.2, 4.1, 5.2])\n",
        "\n",
        "# Calculate MSE without outliers\n",
        "mse_without_outliers = mean_squared_error(true_values , predicted_values) \n",
        "print(f\"MSE without outliers: {mse_without_outliers}\")\n",
        "\n",
        "# Introduce an outlier\n",
        "true_values_with_outlier = np.array([1, 2, 3, 4, 5, 50])\n",
        "predicted_values_with_outlier = np.array([1.1, 1.9, 3.2, 4.1, 5.2, 10])\n",
        "\n",
        "# Calculate MSE with the outlier\n",
        "mse_with_outliers = mean_squared_error(true_values_with_outlier ,predicted_values_with_outlier)\n",
        "print(f\"MSE with outliers: {mse_with_outliers}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mean Absolute Error (MAE):\n",
        "Mean Absolute Error calculates the average of the absolute differences between the predicted and true values. It is less sensitive to outliers compared to MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 0.14000000000000004\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "# Example usage\n",
        "y_true = np.array([1, 2, 3, 4, 5])\n",
        "y_pred = np.array([1.1, 1.9, 3.2, 4.1, 5.2])\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "print(f\"Mean Absolute Error: {mae}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE without outliers: 0.14000000000000004\n",
            "MSE with outliers: 6.783333333333334\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulated true values and predicted values\n",
        "true_values = np.array([1, 2, 3, 4, 5])\n",
        "predicted_values = np.array([1.1, 1.9, 3.2, 4.1, 5.2])\n",
        "\n",
        "# Calculate MAE without outliers\n",
        "mse_without_outliers = mean_absolute_error(true_values , predicted_values) \n",
        "print(f\"MSE without outliers: {mse_without_outliers}\")\n",
        "\n",
        "# Introduce an outlier\n",
        "true_values_with_outlier = np.array([1, 2, 3, 4, 5, 50])\n",
        "predicted_values_with_outlier = np.array([1.1, 1.9, 3.2, 4.1, 5.2, 10])\n",
        "\n",
        "# Calculate MAE with the outlier\n",
        "mse_with_outliers = mean_absolute_error(true_values_with_outlier ,predicted_values_with_outlier)\n",
        "print(f\"MSE with outliers: {mse_with_outliers}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky_SdjMjHKhc"
      },
      "source": [
        "# Cross-Entropy Loss (Binary Classification):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aZcK4CbJZi1"
      },
      "source": [
        "Cross-entropy loss measures the dissimilarity between two probability distributions, in this case, the true labels and predicted probabilities for a binary classification problem. It penalizes the model more if it is confident and incorrect in its predictions, making it suitable for classification tasks where probabilities are required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiMJpmItGuHr",
        "outputId": "209f48e9-d813-477c-a7b6-a5fffd57d5e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary Cross-Entropy: 0.22944289410146546\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    bce = -sum(y_true[i]*math.log(y_pred[i]) + (1-y_true[i])*math.log(1-y_pred[i]) for i in range(n)) / n\n",
        "    return bce\n",
        "\n",
        "y_true = [0, 1, 1, 0, 1]\n",
        "y_pred = [0.1, 0.9, 0.8, 0.3, 0.7]\n",
        "bce = binary_cross_entropy(y_true, y_pred)\n",
        "print(\"Binary Cross-Entropy:\", bce)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoLgvJkrHHVy"
      },
      "source": [
        "## Hinge Loss (Binary Classification):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5wv-32VJfFN"
      },
      "source": [
        "Hinge loss is used in binary classification problems, particularly in support vector machines (SVMs). It measures the distance between the true label (Â±1) and the predicted value. The loss is zero when the prediction is correct, and it linearly increases when the model is less confident or incorrect. It aims to maximize the margin between classes, making it suitable for problems with imbalanced data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmDVfwmMHAPq",
        "outputId": "7a987817-3ecc-44eb-e1fd-8f68a58dffc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hinge Loss: 0.22000000000000003\n"
          ]
        }
      ],
      "source": [
        "def hinge_loss(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    hl = sum(max(0, 1 - y_true[i]*y_pred[i]) for i in range(n)) / n\n",
        "    return hl\n",
        "\n",
        "y_true = [-1, 1, 1, -1, 1]\n",
        "y_pred = [-0.9, 0.8, 0.9, -0.7, 0.6]\n",
        "hl = hinge_loss(y_true, y_pred)\n",
        "print(\"Hinge Loss:\", hl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5EwiBBfHbfz"
      },
      "source": [
        "## Triplet Loss:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG0NJZNwJ_Hb"
      },
      "source": [
        "Triplet loss is used for learning embeddings in similarity-based tasks, where the goal is to learn a representation that places similar data points closer together in the feature space. The loss function takes three inputs: anchor, positive, and negative. The anchor and positive inputs are of the same class, while the negative input is from a different class. The goal is to minimize the distance between the anchor and positive while maximizing the distance between the anchor and negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge2fYUcZHEji",
        "outputId": "67966ef2-2adc-4612-d154-e974733f44b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Triplet Loss: 0\n"
          ]
        }
      ],
      "source": [
        "def triplet_loss(anchor, positive, negative, margin=0.5):\n",
        "    ap_distance = sum((anchor[i] - positive[i])**2 for i in range(len(anchor)))\n",
        "    an_distance = sum((anchor[i] - negative[i])**2 for i in range(len(anchor)))\n",
        "    tl = max(ap_distance - an_distance + margin, 0)\n",
        "    return tl\n",
        "\n",
        "anchor = [1.0, 1.1, 1.2]\n",
        "positive = [1.0, 1.0, 1.3]\n",
        "negative = [2.1, 2.3, 2.4]\n",
        "tl = triplet_loss(anchor, positive, negative)\n",
        "print(\"Triplet Loss:\", tl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCIadlyEIM3g"
      },
      "source": [
        "## Log-Cosh Loss (Regression):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCZ950a9KHZ3"
      },
      "source": [
        "Log-Cosh loss is a smooth approximation of the absolute error for regression problems. It is less sensitive to outliers than the Mean Squared Error, as it does not square the error. The loss function calculates the average logarithm of the hyperbolic cosine of the difference between true and predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AsNy_f4HerC",
        "outputId": "ae9a4316-84be-4bc8-d00c-bb2122e4d6ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log-Cosh Loss: 0.010942242028990806\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def log_cosh_loss(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    lcl = sum(math.log(math.cosh(y_pred[i] - y_true[i])) for i in range(n)) / n\n",
        "    return lcl\n",
        "\n",
        "y_true = [1, 2, 3, 4, 5]\n",
        "y_pred = [1.1, 1.9, 3.2, 3.8, 5.1]\n",
        "lcl = log_cosh_loss(y_true, y_pred)\n",
        "print(\"Log-Cosh Loss:\", lcl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYo-msJCIYyO"
      },
      "source": [
        "# Dice Loss (Binary Segmentation):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nNgClNjKYwE"
      },
      "source": [
        "Dice loss is commonly used for binary segmentation problems, particularly in medical image segmentation. It measures the overlap between the true and predicted segmentation masks. The loss function calculates the similarity coefficient between the true and predicted masks, which ranges between 0 (no overlap) and 1 (perfect overlap). The Dice loss is 1 minus this similarity coefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K28urrKJIW4B",
        "outputId": "d6b16984-a4bd-449d-d333-8160bfca1680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice Loss: 0.15789473407202215\n"
          ]
        }
      ],
      "source": [
        "def dice_loss(y_true, y_pred, smooth=1e-7):\n",
        "    intersection = sum(y_true[i] * y_pred[i] for i in range(len(y_true)))\n",
        "    union = sum(y_true[i] for i in range(len(y_true))) + sum(y_pred[i] for i in range(len(y_pred)))\n",
        "    dl = 1 - (2 * intersection + smooth) / (union + smooth)\n",
        "    return dl\n",
        "\n",
        "y_true = [1, 0, 1, 0, 1]\n",
        "y_pred = [0.9, 0.1, 0.8, 0.2, 0.7]\n",
        "dl = dice_loss(y_true, y_pred)\n",
        "print(\"Dice Loss:\", dl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7UVEPMfIcXm"
      },
      "source": [
        "## Focal Loss (Binary Classification):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BLJSs0UKjYd"
      },
      "source": [
        "Focal loss is an extension of the cross-entropy loss designed to address the class imbalance problem in object detection tasks. It adds a modulating term to the cross-entropy loss, which reduces the loss for well-classified examples, allowing the model to focus on hard-to-classify instances. It has two hyperparameters: alpha, which balances the importance of positive and negative examples, and gamma, which controls the focus on hard examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrRbmOhDIaYQ",
        "outputId": "1f5508c7-9e98-48ac-de3a-81073d81982b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Focal Loss: 0.007077157124841256\n"
          ]
        }
      ],
      "source": [
        "def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
        "    n = len(y_true)\n",
        "    fl = -sum(alpha * y_true[i] * (1 - y_pred[i])**gamma * math.log(y_pred[i]) +\n",
        "              (1 - alpha) * (1 - y_true[i]) * y_pred[i]**gamma * math.log(1 - y_pred[i]) for i in range(n)) / n\n",
        "    return fl\n",
        "\n",
        "y_true = [0, 1, 1, 0, 1]\n",
        "y_pred = [0.1, 0.9, 0.8, 0.3, 0.7]\n",
        "fl = focal_loss(y_true, y_pred)\n",
        "print(\"Focal Loss:\", fl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQAIHLD-Ihwf"
      },
      "source": [
        "## Kullback-Leibler (KL) Divergence Loss:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdEu83CvKzE8"
      },
      "source": [
        "KL divergence loss measures the dissimilarity between two probability distributions, which can be useful in tasks such as unsupervised learning or generative modeling. It calculates the average difference in the logarithm of probabilities between the true and predicted distributions. KL divergence is not symmetric, meaning that the order of the true and predicted distributions affects the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AiTZl2rIe-S",
        "outputId": "56cc0d99-ee1f-4a69-f219-fc87ed579db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kullback-Leibler Divergence Loss: 0.014145256669114606\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def kl_divergence(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    kld = sum(y_true[i] * math.log(y_true[i] / y_pred[i]) for i in range(n)) / n\n",
        "    return kld\n",
        "\n",
        "y_true = [0.2, 0.3, 0.1, 0.4]\n",
        "y_pred = [0.25, 0.35, 0.15, 0.25]\n",
        "kld = kl_divergence(y_true, y_pred)\n",
        "print(\"Kullback-Leibler Divergence Loss:\", kld)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Huber Loss:\n",
        "Huber Loss is a combination of MSE and MAE. It is less sensitive to outliers than MSE while still maintaining differentiability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7Vi3k0bwIlHC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Huber Loss: 0.01100000000000001\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta=1.0):\n",
        "    error = y_true - y_pred\n",
        "    abs_error = np.abs(error)\n",
        "    squared_error = 0.5 * (error ** 2)\n",
        "    absolute_error = delta * (abs_error - 0.5 * delta)\n",
        "    return np.mean(np.where(abs_error <= delta, squared_error, absolute_error))\n",
        "\n",
        "# Example usage\n",
        "y_true = np.array([1, 2, 3, 4, 5])\n",
        "y_pred = np.array([1.1, 1.9, 3.2, 4.1, 5.2])\n",
        "\n",
        "huber = huber_loss(y_true, y_pred)\n",
        "print(f\"Huber Loss: {huber}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HUBER without outliers: 0.01100000000000001\n",
            "MSE with outliers: 6.5925\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulated true values and predicted values\n",
        "true_values = np.array([1, 2, 3, 4, 5])\n",
        "predicted_values = np.array([1.1, 1.9, 3.2, 4.1, 5.2])\n",
        "\n",
        "# Calculate HUBER without outliers\n",
        "huber_without_outliers = huber_loss(true_values , predicted_values) \n",
        "print(f\"HUBER without outliers: {huber_without_outliers}\")\n",
        "\n",
        "# Introduce an outlier\n",
        "true_values_with_outlier = np.array([1, 2, 3, 4, 5, 50])\n",
        "predicted_values_with_outlier = np.array([1.1, 1.9, 3.2, 4.1, 5.2, 10])\n",
        "\n",
        "# Calculate HUBER with the outlier\n",
        "huber_with_outliers = huber_loss(true_values_with_outlier ,predicted_values_with_outlier)\n",
        "print(f\"MSE with outliers: {huber_with_outliers}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMxfPiwYswqtPtFCdC/9VPg",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
