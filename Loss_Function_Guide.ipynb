{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxfPiwYswqtPtFCdC/9VPg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnyanshwalwadkar/Adv-Deep-Learning/blob/main/Loss_Function_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Title: The Art of Loss Function Selection in Deep Learning: A Comprehensive Guide\n",
        "\n"
      ],
      "metadata": {
        "id": "nVWC8dR-NAKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mean Squared Error (MSE) Loss:"
      ],
      "metadata": {
        "id": "TgH1GFmKHLo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE is a widely used loss function for regression problems. It calculates the average squared difference between the true and predicted values. This loss function is sensitive to outliers, as the error gets squared, making it suitable for problems where larger errors should be penalized more heavily."
      ],
      "metadata": {
        "id": "1gSBE0dcI2a3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKo1Fe4_GXw3",
        "outputId": "1970aa52-0fc7-4148-c8f0-bbd992269f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.02200000000000002\n"
          ]
        }
      ],
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    mse = sum((y_true[i] - y_pred[i])**2 for i in range(n)) / n\n",
        "    return mse\n",
        "\n",
        "y_true = [1, 2, 3, 4, 5]\n",
        "y_pred = [1.1, 1.9, 3.2, 3.8, 5.1]\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "print(\"MSE:\", mse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Entropy Loss (Binary Classification):"
      ],
      "metadata": {
        "id": "ky_SdjMjHKhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-entropy loss measures the dissimilarity between two probability distributions, in this case, the true labels and predicted probabilities for a binary classification problem. It penalizes the model more if it is confident and incorrect in its predictions, making it suitable for classification tasks where probabilities are required."
      ],
      "metadata": {
        "id": "6aZcK4CbJZi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    bce = -sum(y_true[i]*math.log(y_pred[i]) + (1-y_true[i])*math.log(1-y_pred[i]) for i in range(n)) / n\n",
        "    return bce\n",
        "\n",
        "y_true = [0, 1, 1, 0, 1]\n",
        "y_pred = [0.1, 0.9, 0.8, 0.3, 0.7]\n",
        "bce = binary_cross_entropy(y_true, y_pred)\n",
        "print(\"Binary Cross-Entropy:\", bce)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiMJpmItGuHr",
        "outputId": "209f48e9-d813-477c-a7b6-a5fffd57d5e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Cross-Entropy: 0.22944289410146546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hinge Loss (Binary Classification):"
      ],
      "metadata": {
        "id": "EoLgvJkrHHVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hinge loss is used in binary classification problems, particularly in support vector machines (SVMs). It measures the distance between the true label (Â±1) and the predicted value. The loss is zero when the prediction is correct, and it linearly increases when the model is less confident or incorrect. It aims to maximize the margin between classes, making it suitable for problems with imbalanced data."
      ],
      "metadata": {
        "id": "n5wv-32VJfFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hinge_loss(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    hl = sum(max(0, 1 - y_true[i]*y_pred[i]) for i in range(n)) / n\n",
        "    return hl\n",
        "\n",
        "y_true = [-1, 1, 1, -1, 1]\n",
        "y_pred = [-0.9, 0.8, 0.9, -0.7, 0.6]\n",
        "hl = hinge_loss(y_true, y_pred)\n",
        "print(\"Hinge Loss:\", hl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmDVfwmMHAPq",
        "outputId": "7a987817-3ecc-44eb-e1fd-8f68a58dffc7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hinge Loss: 0.22000000000000003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Triplet Loss:"
      ],
      "metadata": {
        "id": "o5EwiBBfHbfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Triplet loss is used for learning embeddings in similarity-based tasks, where the goal is to learn a representation that places similar data points closer together in the feature space. The loss function takes three inputs: anchor, positive, and negative. The anchor and positive inputs are of the same class, while the negative input is from a different class. The goal is to minimize the distance between the anchor and positive while maximizing the distance between the anchor and negative."
      ],
      "metadata": {
        "id": "hG0NJZNwJ_Hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def triplet_loss(anchor, positive, negative, margin=0.5):\n",
        "    ap_distance = sum((anchor[i] - positive[i])**2 for i in range(len(anchor)))\n",
        "    an_distance = sum((anchor[i] - negative[i])**2 for i in range(len(anchor)))\n",
        "    tl = max(ap_distance - an_distance + margin, 0)\n",
        "    return tl\n",
        "\n",
        "anchor = [1.0, 1.1, 1.2]\n",
        "positive = [1.0, 1.0, 1.3]\n",
        "negative = [2.1, 2.3, 2.4]\n",
        "tl = triplet_loss(anchor, positive, negative)\n",
        "print(\"Triplet Loss:\", tl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge2fYUcZHEji",
        "outputId": "67966ef2-2adc-4612-d154-e974733f44b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triplet Loss: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log-Cosh Loss (Regression):"
      ],
      "metadata": {
        "id": "BCIadlyEIM3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log-Cosh loss is a smooth approximation of the absolute error for regression problems. It is less sensitive to outliers than the Mean Squared Error, as it does not square the error. The loss function calculates the average logarithm of the hyperbolic cosine of the difference between true and predicted values."
      ],
      "metadata": {
        "id": "bCZ950a9KHZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def log_cosh_loss(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    lcl = sum(math.log(math.cosh(y_pred[i] - y_true[i])) for i in range(n)) / n\n",
        "    return lcl\n",
        "\n",
        "y_true = [1, 2, 3, 4, 5]\n",
        "y_pred = [1.1, 1.9, 3.2, 3.8, 5.1]\n",
        "lcl = log_cosh_loss(y_true, y_pred)\n",
        "print(\"Log-Cosh Loss:\", lcl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AsNy_f4HerC",
        "outputId": "ae9a4316-84be-4bc8-d00c-bb2122e4d6ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log-Cosh Loss: 0.010942242028990806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dice Loss (Binary Segmentation):"
      ],
      "metadata": {
        "id": "IYo-msJCIYyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dice loss is commonly used for binary segmentation problems, particularly in medical image segmentation. It measures the overlap between the true and predicted segmentation masks. The loss function calculates the similarity coefficient between the true and predicted masks, which ranges between 0 (no overlap) and 1 (perfect overlap). The Dice loss is 1 minus this similarity coefficient."
      ],
      "metadata": {
        "id": "5nNgClNjKYwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_loss(y_true, y_pred, smooth=1e-7):\n",
        "    intersection = sum(y_true[i] * y_pred[i] for i in range(len(y_true)))\n",
        "    union = sum(y_true[i] for i in range(len(y_true))) + sum(y_pred[i] for i in range(len(y_pred)))\n",
        "    dl = 1 - (2 * intersection + smooth) / (union + smooth)\n",
        "    return dl\n",
        "\n",
        "y_true = [1, 0, 1, 0, 1]\n",
        "y_pred = [0.9, 0.1, 0.8, 0.2, 0.7]\n",
        "dl = dice_loss(y_true, y_pred)\n",
        "print(\"Dice Loss:\", dl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K28urrKJIW4B",
        "outputId": "d6b16984-a4bd-449d-d333-8160bfca1680"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dice Loss: 0.15789473407202215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Focal Loss (Binary Classification):"
      ],
      "metadata": {
        "id": "h7UVEPMfIcXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focal loss is an extension of the cross-entropy loss designed to address the class imbalance problem in object detection tasks. It adds a modulating term to the cross-entropy loss, which reduces the loss for well-classified examples, allowing the model to focus on hard-to-classify instances. It has two hyperparameters: alpha, which balances the importance of positive and negative examples, and gamma, which controls the focus on hard examples."
      ],
      "metadata": {
        "id": "9BLJSs0UKjYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
        "    n = len(y_true)\n",
        "    fl = -sum(alpha * y_true[i] * (1 - y_pred[i])**gamma * math.log(y_pred[i]) +\n",
        "              (1 - alpha) * (1 - y_true[i]) * y_pred[i]**gamma * math.log(1 - y_pred[i]) for i in range(n)) / n\n",
        "    return fl\n",
        "\n",
        "y_true = [0, 1, 1, 0, 1]\n",
        "y_pred = [0.1, 0.9, 0.8, 0.3, 0.7]\n",
        "fl = focal_loss(y_true, y_pred)\n",
        "print(\"Focal Loss:\", fl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrRbmOhDIaYQ",
        "outputId": "1f5508c7-9e98-48ac-de3a-81073d81982b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Focal Loss: 0.007077157124841256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kullback-Leibler (KL) Divergence Loss:"
      ],
      "metadata": {
        "id": "mQAIHLD-Ihwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KL divergence loss measures the dissimilarity between two probability distributions, which can be useful in tasks such as unsupervised learning or generative modeling. It calculates the average difference in the logarithm of probabilities between the true and predicted distributions. KL divergence is not symmetric, meaning that the order of the true and predicted distributions affects the result."
      ],
      "metadata": {
        "id": "NdEu83CvKzE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def kl_divergence(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    kld = sum(y_true[i] * math.log(y_true[i] / y_pred[i]) for i in range(n)) / n\n",
        "    return kld\n",
        "\n",
        "y_true = [0.2, 0.3, 0.1, 0.4]\n",
        "y_pred = [0.25, 0.35, 0.15, 0.25]\n",
        "kld = kl_divergence(y_true, y_pred)\n",
        "print(\"Kullback-Leibler Divergence Loss:\", kld)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AiTZl2rIe-S",
        "outputId": "56cc0d99-ee1f-4a69-f219-fc87ed579db7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kullback-Leibler Divergence Loss: 0.014145256669114606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Vi3k0bwIlHC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}